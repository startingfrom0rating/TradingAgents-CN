"""
è‚¡ç¥¨åˆ†ææœåŠ¡
å°†ç°æœ‰çš„TradingAgentsåˆ†æåŠŸèƒ½åŒ…è£…æˆAPIæœåŠ¡
"""

import asyncio
import uuid
import json
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG
from app.services.simple_analysis_service import create_analysis_config, get_provider_by_model_name
from app.models.analysis import (
    AnalysisParameters, AnalysisResult, AnalysisTask, AnalysisBatch,
    AnalysisStatus, BatchStatus, SingleAnalysisRequest, BatchAnalysisRequest
)
from app.models.user import PyObjectId
from bson import ObjectId
from app.core.database import get_mongo_db
from app.core.redis_client import get_redis_service, RedisKeys
from app.services.queue_service import QueueService
from app.core.database import get_redis_client

import logging
logger = logging.getLogger(__name__)


class AnalysisService:
    """è‚¡ç¥¨åˆ†ææœåŠ¡ç±»"""
    
    def __init__(self):
        # è·å–Rediså®¢æˆ·ç«¯
        redis_client = get_redis_client()
        self.queue_service = QueueService(redis_client)
        self._trading_graph_cache = {}

    def _convert_user_id(self, user_id: str) -> PyObjectId:
        """å°†å­—ç¬¦ä¸²ç”¨æˆ·IDè½¬æ¢ä¸ºPyObjectId"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ç”¨æˆ·ID: {user_id} (ç±»å‹: {type(user_id)})")

            # å¦‚æœæ˜¯adminç”¨æˆ·ï¼Œä½¿ç”¨å›ºå®šçš„ObjectId
            if user_id == "admin":
                # ä½¿ç”¨å›ºå®šçš„ObjectIdä½œä¸ºadminç”¨æˆ·ID
                admin_object_id = ObjectId("507f1f77bcf86cd799439011")
                logger.info(f"ğŸ”„ è½¬æ¢adminç”¨æˆ·ID: {user_id} -> {admin_object_id}")
                return PyObjectId(admin_object_id)
            else:
                # å°è¯•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºObjectId
                object_id = ObjectId(user_id)
                logger.info(f"ğŸ”„ è½¬æ¢ç”¨æˆ·ID: {user_id} -> {object_id}")
                return PyObjectId(object_id)
        except Exception as e:
            logger.error(f"âŒ ç”¨æˆ·IDè½¬æ¢å¤±è´¥: {user_id} -> {e}")
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ObjectId
            new_object_id = ObjectId()
            logger.warning(f"âš ï¸ ç”Ÿæˆæ–°çš„ç”¨æˆ·ID: {new_object_id}")
            return PyObjectId(new_object_id)
    
    def _get_trading_graph(self, config: Dict[str, Any]) -> TradingAgentsGraph:
        """è·å–æˆ–åˆ›å»ºTradingAgentså›¾å®ä¾‹ï¼ˆå¸¦ç¼“å­˜ï¼‰- ä¸å•è‚¡åˆ†æä¿æŒä¸€è‡´"""
        config_key = json.dumps(config, sort_keys=True)

        if config_key not in self._trading_graph_cache:
            # ç›´æ¥ä½¿ç”¨å®Œæ•´é…ç½®ï¼Œä¸å†åˆå¹¶DEFAULT_CONFIGï¼ˆå› ä¸ºcreate_analysis_configå·²ç»å¤„ç†äº†ï¼‰
            # è¿™ä¸å•è‚¡åˆ†ææœåŠ¡å’Œwebç›®å½•çš„æ–¹å¼ä¸€è‡´
            self._trading_graph_cache[config_key] = TradingAgentsGraph(
                selected_analysts=config.get("selected_analysts", ["market", "fundamentals"]),
                debug=config.get("debug", False),
                config=config
            )

            logger.info(f"åˆ›å»ºæ–°çš„TradingAgentså®ä¾‹: {config.get('llm_provider', 'default')}")

        return self._trading_graph_cache[config_key]

    def _execute_analysis_sync(self, task: AnalysisTask) -> AnalysisResult:
        """åŒæ­¥æ‰§è¡Œåˆ†æä»»åŠ¡ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
        try:
            logger.info(f"ğŸ”„ [çº¿ç¨‹æ± ] å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.stock_code}")

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            from app.core.unified_config import unified_config

            quick_model = getattr(task.parameters, 'quick_analysis_model', None) or unified_config.get_quick_analysis_model()
            deep_model = getattr(task.parameters, 'deep_analysis_model', None) or unified_config.get_deep_analysis_model()

            # æ ¹æ®æ¨¡å‹åç§°åŠ¨æ€æŸ¥æ‰¾ä¾›åº”å•†ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
            llm_provider = "dashscope"  # é»˜è®¤ä½¿ç”¨dashscope

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            from app.services.simple_analysis_service import create_analysis_config
            config = create_analysis_config(
                research_depth=task.parameters.research_depth,
                selected_analysts=task.parameters.selected_analysts or ["market", "fundamentals"],
                quick_model=quick_model,
                deep_model=deep_model,
                llm_provider=llm_provider,
                market_type=getattr(task.parameters, 'market_type', "Aè‚¡")
            )

            # è·å–TradingAgentså®ä¾‹
            trading_graph = self._get_trading_graph(config)

            # æ‰§è¡Œåˆ†æ
            from datetime import timezone
            start_time = datetime.now(timezone.utc)
            analysis_date = task.parameters.analysis_date or datetime.now().strftime("%Y-%m-%d")

            # è°ƒç”¨ç°æœ‰çš„åˆ†ææ–¹æ³•ï¼ˆåŒæ­¥è°ƒç”¨ï¼‰
            _, decision = trading_graph.propagate(task.stock_code, analysis_date)

            execution_time = (datetime.now(timezone.utc) - start_time).total_seconds()

            # æ„å»ºç»“æœ
            result = AnalysisResult(
                analysis_id=str(uuid.uuid4()),
                summary=decision.get("summary", ""),
                recommendation=decision.get("recommendation", ""),
                confidence_score=decision.get("confidence_score", 0.0),
                risk_level=decision.get("risk_level", "ä¸­ç­‰"),
                key_points=decision.get("key_points", []),
                detailed_analysis=decision,
                execution_time=execution_time,
                tokens_used=decision.get("tokens_used", 0)
            )

            logger.info(f"âœ… [çº¿ç¨‹æ± ] åˆ†æä»»åŠ¡å®Œæˆ: {task.task_id} - è€—æ—¶{execution_time:.2f}ç§’")
            return result

        except Exception as e:
            logger.error(f"âŒ [çº¿ç¨‹æ± ] æ‰§è¡Œåˆ†æä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")
            raise

    async def _execute_single_analysis_async(self, task: AnalysisTask):
        """å¼‚æ­¥æ‰§è¡Œå•è‚¡åˆ†æä»»åŠ¡ï¼ˆåœ¨åå°è¿è¡Œï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹ï¼‰"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.stock_code}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
            await self._update_task_status(task.task_id, AnalysisStatus.PROCESSING, 10)

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œåˆ†æï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            import asyncio
            import concurrent.futures

            loop = asyncio.get_event_loop()

            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨è¿è¡ŒåŒæ­¥çš„åˆ†æä»£ç 
            with concurrent.futures.ThreadPoolExecutor() as executor:
                result = await loop.run_in_executor(
                    executor,
                    self._execute_analysis_sync,
                    task
                )

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            await self._update_task_status(task.task_id, AnalysisStatus.COMPLETED, 100, result)

            logger.info(f"âœ… åˆ†æä»»åŠ¡å®Œæˆ: {task.task_id}")

        except Exception as e:
            logger.error(f"âŒ åˆ†æä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            await self._update_task_status(task.task_id, AnalysisStatus.FAILED, 0, str(e))

    async def submit_single_analysis(
        self,
        user_id: str,
        request: SingleAnalysisRequest
    ) -> Dict[str, Any]:
        """æäº¤å•è‚¡åˆ†æä»»åŠ¡"""
        try:
            logger.info(f"ğŸ“ å¼€å§‹æäº¤å•è‚¡åˆ†æä»»åŠ¡")
            logger.info(f"ğŸ‘¤ ç”¨æˆ·ID: {user_id} (ç±»å‹: {type(user_id)})")
            logger.info(f"ğŸ“Š è‚¡ç¥¨ä»£ç : {request.stock_code}")
            logger.info(f"âš™ï¸ åˆ†æå‚æ•°: {request.parameters}")

            # ç”Ÿæˆä»»åŠ¡ID
            task_id = str(uuid.uuid4())
            logger.info(f"ğŸ†” ç”Ÿæˆä»»åŠ¡ID: {task_id}")

            # è½¬æ¢ç”¨æˆ·ID
            converted_user_id = self._convert_user_id(user_id)
            logger.info(f"ğŸ”„ è½¬æ¢åçš„ç”¨æˆ·ID: {converted_user_id} (ç±»å‹: {type(converted_user_id)})")

            # åˆ›å»ºåˆ†æä»»åŠ¡
            logger.info(f"ğŸ—ï¸ å¼€å§‹åˆ›å»ºAnalysisTaskå¯¹è±¡...")
            task = AnalysisTask(
                task_id=task_id,
                user_id=converted_user_id,
                stock_code=request.stock_code,
                parameters=request.parameters or AnalysisParameters(),
                status=AnalysisStatus.PENDING
            )
            logger.info(f"âœ… AnalysisTaskå¯¹è±¡åˆ›å»ºæˆåŠŸ")
            
            # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
            logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“...")
            db = get_mongo_db()
            task_dict = task.model_dump(by_alias=True)
            logger.info(f"ğŸ“„ ä»»åŠ¡å­—å…¸: {task_dict}")
            await db.analysis_tasks.insert_one(task_dict)
            logger.info(f"âœ… ä»»åŠ¡å·²ä¿å­˜åˆ°æ•°æ®åº“")

            # å•è‚¡åˆ†æï¼šç›´æ¥åœ¨åå°æ‰§è¡Œï¼ˆä¸é˜»å¡APIå“åº”ï¼‰
            logger.info(f"ğŸš€ å¼€å§‹åœ¨åå°æ‰§è¡Œåˆ†æä»»åŠ¡...")

            # åˆ›å»ºåå°ä»»åŠ¡ï¼Œä¸ç­‰å¾…å®Œæˆ
            import asyncio
            background_task = asyncio.create_task(
                self._execute_single_analysis_async(task)
            )

            # ä¸ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œè®©å®ƒåœ¨åå°è¿è¡Œ
            logger.info(f"âœ… åå°ä»»åŠ¡å·²å¯åŠ¨ï¼Œä»»åŠ¡ID: {task_id}")

            logger.info(f"ğŸ‰ å•è‚¡åˆ†æä»»åŠ¡æäº¤å®Œæˆ: {task_id} - {request.stock_code}")

            return {
                "task_id": task_id,
                "stock_code": request.stock_code,
                "status": AnalysisStatus.PENDING,
                "message": "ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨"
            }
            
        except Exception as e:
            logger.error(f"æäº¤å•è‚¡åˆ†æä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def submit_batch_analysis(
        self, 
        user_id: str, 
        request: BatchAnalysisRequest
    ) -> Dict[str, Any]:
        """æäº¤æ‰¹é‡åˆ†æä»»åŠ¡"""
        try:
            # ç”Ÿæˆæ‰¹æ¬¡ID
            batch_id = str(uuid.uuid4())
            
            # è½¬æ¢ç”¨æˆ·ID
            converted_user_id = self._convert_user_id(user_id)

            # åˆ›å»ºæ‰¹æ¬¡è®°å½•
            batch = AnalysisBatch(
                batch_id=batch_id,
                user_id=converted_user_id,
                title=request.title,
                description=request.description,
                total_tasks=len(request.stock_codes),
                parameters=request.parameters or AnalysisParameters(),
                status=BatchStatus.PENDING
            )

            # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
            tasks = []
            for stock_code in request.stock_codes:
                task_id = str(uuid.uuid4())
                task = AnalysisTask(
                    task_id=task_id,
                    batch_id=batch_id,
                    user_id=converted_user_id,
                    stock_code=stock_code,
                    parameters=batch.parameters,
                    status=AnalysisStatus.PENDING
                )
                tasks.append(task)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            db = get_mongo_db()
            await db.analysis_batches.insert_one(batch.dict(by_alias=True))
            await db.analysis_tasks.insert_many([task.dict(by_alias=True) for task in tasks])
            
            # æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
            for task in tasks:
                # å‡†å¤‡é˜Ÿåˆ—å‚æ•°ï¼ˆç›´æ¥ä¼ é€’åˆ†æå‚æ•°ï¼Œä¸åµŒå¥—ï¼‰
                queue_params = task.parameters.dict() if task.parameters else {}

                # æ·»åŠ ä»»åŠ¡å…ƒæ•°æ®
                queue_params.update({
                    "task_id": task.task_id,
                    "stock_code": task.stock_code,
                    "user_id": str(task.user_id),
                    "batch_id": task.batch_id,
                    "created_at": task.created_at.isoformat() if task.created_at else None
                })

                # è°ƒç”¨é˜Ÿåˆ—æœåŠ¡
                await self.queue_service.enqueue_task(
                    user_id=str(converted_user_id),
                    symbol=task.stock_code,
                    params=queue_params,
                    batch_id=task.batch_id
                )
            
            logger.info(f"æ‰¹é‡åˆ†æä»»åŠ¡å·²æäº¤: {batch_id} - {len(tasks)}ä¸ªè‚¡ç¥¨")
            
            return {
                "batch_id": batch_id,
                "total_tasks": len(tasks),
                "status": BatchStatus.PENDING,
                "message": f"å·²æäº¤{len(tasks)}ä¸ªåˆ†æä»»åŠ¡åˆ°é˜Ÿåˆ—"
            }
            
        except Exception as e:
            logger.error(f"æäº¤æ‰¹é‡åˆ†æä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def execute_analysis_task(
        self, 
        task: AnalysisTask,
        progress_callback: Optional[Callable[[int, str], None]] = None
    ) -> AnalysisResult:
        """æ‰§è¡Œå•ä¸ªåˆ†æä»»åŠ¡"""
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.stock_code}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self._update_task_status(task.task_id, AnalysisStatus.PROCESSING, 0)
            
            if progress_callback:
                progress_callback(10, "åˆå§‹åŒ–åˆ†æå¼•æ“...")
            
            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½® - ä¸å•è‚¡åˆ†æä¿æŒä¸€è‡´
            from app.core.unified_config import unified_config

            quick_model = getattr(task.parameters, 'quick_analysis_model', None) or unified_config.get_quick_analysis_model()
            deep_model = getattr(task.parameters, 'deep_analysis_model', None) or unified_config.get_deep_analysis_model()

            # æ ¹æ®æ¨¡å‹åç§°åŠ¨æ€æŸ¥æ‰¾ä¾›åº”å•†
            llm_provider = await get_provider_by_model_name(quick_model)

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            config = create_analysis_config(
                research_depth=task.parameters.research_depth,
                selected_analysts=task.parameters.selected_analysts or ["market", "fundamentals"],
                quick_model=quick_model,
                deep_model=deep_model,
                llm_provider=llm_provider,
                market_type=getattr(task.parameters, 'market_type', "Aè‚¡")
            )
            
            if progress_callback:
                progress_callback(30, "åˆ›å»ºåˆ†æå›¾...")
            
            # è·å–TradingAgentså®ä¾‹
            trading_graph = self._get_trading_graph(config)
            
            if progress_callback:
                progress_callback(50, "æ‰§è¡Œè‚¡ç¥¨åˆ†æ...")
            
            # æ‰§è¡Œåˆ†æ
            start_time = datetime.utcnow()
            analysis_date = task.parameters.analysis_date or datetime.now().strftime("%Y-%m-%d")
            
            # è°ƒç”¨ç°æœ‰çš„åˆ†ææ–¹æ³•
            _, decision = trading_graph.propagate(task.stock_code, analysis_date)
            
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            
            if progress_callback:
                progress_callback(80, "å¤„ç†åˆ†æç»“æœ...")
            
            # æ„å»ºç»“æœ
            result = AnalysisResult(
                analysis_id=str(uuid.uuid4()),
                summary=decision.get("summary", ""),
                recommendation=decision.get("recommendation", ""),
                confidence_score=decision.get("confidence_score", 0.0),
                risk_level=decision.get("risk_level", "ä¸­ç­‰"),
                key_points=decision.get("key_points", []),
                detailed_analysis=decision,
                execution_time=execution_time,
                tokens_used=decision.get("tokens_used", 0)
            )
            
            if progress_callback:
                progress_callback(100, "åˆ†æå®Œæˆ")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self._update_task_status(task.task_id, AnalysisStatus.COMPLETED, 100, result)
            
            logger.info(f"åˆ†æä»»åŠ¡å®Œæˆ: {task.task_id} - è€—æ—¶{execution_time:.2f}ç§’")
            
            return result
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œåˆ†æä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            error_result = AnalysisResult(error_message=str(e))
            await self._update_task_status(task.task_id, AnalysisStatus.FAILED, 0, error_result)
            
            raise
    
    async def _update_task_status(
        self, 
        task_id: str, 
        status: AnalysisStatus, 
        progress: int,
        result: Optional[AnalysisResult] = None
    ):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        try:
            db = get_mongo_db()
            redis_service = get_redis_service()
            
            # å‡†å¤‡æ›´æ–°æ•°æ®
            update_data = {
                "status": status,
                "progress": progress,
                "updated_at": datetime.utcnow()
            }
            
            if status == AnalysisStatus.PROCESSING and "started_at" not in update_data:
                update_data["started_at"] = datetime.utcnow()
            elif status in [AnalysisStatus.COMPLETED, AnalysisStatus.FAILED]:
                update_data["completed_at"] = datetime.utcnow()
                if result:
                    update_data["result"] = result.dict()
            
            # æ›´æ–°æ•°æ®åº“
            await db.analysis_tasks.update_one(
                {"task_id": task_id},
                {"$set": update_data}
            )
            
            # æ›´æ–°Redisç¼“å­˜
            progress_key = RedisKeys.TASK_PROGRESS.format(task_id=task_id)
            await redis_service.set_json(progress_key, {
                "task_id": task_id,
                "status": status,
                "progress": progress,
                "updated_at": datetime.utcnow().isoformat()
            }, ttl=3600)
            
        except Exception as e:
            logger.error(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_id} - {e}")
    
    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        try:
            # å…ˆä»Redisç¼“å­˜è·å–
            redis_service = get_redis_service()
            progress_key = RedisKeys.TASK_PROGRESS.format(task_id=task_id)
            cached_status = await redis_service.get_json(progress_key)
            
            if cached_status:
                return cached_status
            
            # ä»æ•°æ®åº“è·å–
            db = get_mongo_db()
            task = await db.analysis_tasks.find_one({"task_id": task_id})
            
            if task:
                return {
                    "task_id": task_id,
                    "status": task.get("status"),
                    "progress": task.get("progress", 0),
                    "updated_at": task.get("updated_at", "").isoformat() if task.get("updated_at") else None
                }
            
            return None
            
        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_id} - {e}")
            return None
    
    async def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self._update_task_status(task_id, AnalysisStatus.CANCELLED, 0)
            
            # ä»é˜Ÿåˆ—ä¸­ç§»é™¤ï¼ˆå¦‚æœè¿˜åœ¨é˜Ÿåˆ—ä¸­ï¼‰
            await self.queue_service.remove_task(task_id)
            
            logger.info(f"ä»»åŠ¡å·²å–æ¶ˆ: {task_id}")
            return True
            
        except Exception as e:
            logger.error(f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {task_id} - {e}")
            return False


# å…¨å±€åˆ†ææœåŠ¡å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
analysis_service: Optional[AnalysisService] = None


def get_analysis_service() -> AnalysisService:
    """è·å–åˆ†ææœåŠ¡å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global analysis_service
    if analysis_service is None:
        analysis_service = AnalysisService()
    return analysis_service
