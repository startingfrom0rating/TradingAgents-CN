"""
ç®€åŒ–çš„è‚¡ç¥¨åˆ†ææœåŠ¡
ç›´æ¥è°ƒç”¨ç°æœ‰çš„ TradingAgents åˆ†æåŠŸèƒ½
"""

import asyncio
import uuid
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG
from app.models.analysis import (
    AnalysisTask, AnalysisStatus, SingleAnalysisRequest, AnalysisParameters
)
from app.models.user import PyObjectId
from bson import ObjectId
from app.core.database import get_mongo_db
from app.services.config_service import ConfigService
from app.services.memory_state_manager import get_memory_state_manager, TaskStatus

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger("app.services.simple_analysis_service")

# é…ç½®æœåŠ¡å®ä¾‹
config_service = ConfigService()


async def get_provider_by_model_name(model_name: str) -> str:
    """
    æ ¹æ®æ¨¡å‹åç§°ä»æ•°æ®åº“é…ç½®ä¸­æŸ¥æ‰¾å¯¹åº”çš„ä¾›åº”å•†

    Args:
        model_name: æ¨¡å‹åç§°ï¼Œå¦‚ 'qwen-turbo', 'gpt-4' ç­‰

    Returns:
        str: ä¾›åº”å•†åç§°ï¼Œå¦‚ 'dashscope', 'openai' ç­‰
    """
    try:
        # ä»é…ç½®æœåŠ¡è·å–ç³»ç»Ÿé…ç½®
        system_config = await config_service.get_system_config()
        if not system_config or not system_config.llm_configs:
            logger.warning(f"âš ï¸ ç³»ç»Ÿé…ç½®ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ä¾›åº”å•†æ˜ å°„")
            return _get_default_provider_by_model(model_name)

        # åœ¨LLMé…ç½®ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ¨¡å‹
        for llm_config in system_config.llm_configs:
            if llm_config.model_name == model_name:
                provider = llm_config.provider.value if hasattr(llm_config.provider, 'value') else str(llm_config.provider)
                logger.info(f"âœ… ä»æ•°æ®åº“æ‰¾åˆ°æ¨¡å‹ {model_name} çš„ä¾›åº”å•†: {provider}")
                return provider

        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„
        logger.warning(f"âš ï¸ æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æ¨¡å‹ {model_name}ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„")
        return _get_default_provider_by_model(model_name)

    except Exception as e:
        logger.error(f"âŒ æŸ¥æ‰¾æ¨¡å‹ä¾›åº”å•†å¤±è´¥: {e}")
        return _get_default_provider_by_model(model_name)


def _get_default_provider_by_model(model_name: str) -> str:
    """
    æ ¹æ®æ¨¡å‹åç§°è¿”å›é»˜è®¤çš„ä¾›åº”å•†æ˜ å°„
    è¿™æ˜¯ä¸€ä¸ªåå¤‡æ–¹æ¡ˆï¼Œå½“æ•°æ®åº“æŸ¥è¯¢å¤±è´¥æ—¶ä½¿ç”¨
    """
    # æ¨¡å‹åç§°åˆ°ä¾›åº”å•†çš„é»˜è®¤æ˜ å°„
    model_provider_map = {
        # é˜¿é‡Œç™¾ç‚¼ (DashScope)
        'qwen-turbo': 'dashscope',
        'qwen-plus': 'dashscope',
        'qwen-max': 'dashscope',
        'qwen-plus-latest': 'dashscope',
        'qwen-max-longcontext': 'dashscope',

        # OpenAI
        'gpt-3.5-turbo': 'openai',
        'gpt-4': 'openai',
        'gpt-4-turbo': 'openai',
        'gpt-4o': 'openai',
        'gpt-4o-mini': 'openai',

        # Google
        'gemini-pro': 'google',
        'gemini-2.0-flash': 'google',
        'gemini-2.0-flash-thinking-exp': 'google',

        # DeepSeek
        'deepseek-chat': 'deepseek',
        'deepseek-coder': 'deepseek',

        # æ™ºè°±AI
        'glm-4': 'zhipu',
        'glm-3-turbo': 'zhipu',
        'chatglm3-6b': 'zhipu'
    }

    provider = model_provider_map.get(model_name, 'dashscope')  # é»˜è®¤ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼
    logger.info(f"ğŸ”§ ä½¿ç”¨é»˜è®¤æ˜ å°„: {model_name} -> {provider}")
    return provider


def create_analysis_config(
    research_depth: str,
    selected_analysts: list,
    quick_model: str,
    deep_model: str,
    llm_provider: str,
    market_type: str = "Aè‚¡"
) -> dict:
    """
    åˆ›å»ºåˆ†æé…ç½® - å®Œå…¨å¤åˆ¶webç›®å½•çš„é…ç½®é€»è¾‘

    Args:
        research_depth: ç ”ç©¶æ·±åº¦ ("å¿«é€Ÿ", "æ ‡å‡†", "æ·±åº¦")
        selected_analysts: é€‰ä¸­çš„åˆ†æå¸ˆåˆ—è¡¨
        quick_model: å¿«é€Ÿåˆ†ææ¨¡å‹
        deep_model: æ·±åº¦åˆ†ææ¨¡å‹
        llm_provider: LLMä¾›åº”å•†
        market_type: å¸‚åœºç±»å‹

    Returns:
        dict: å®Œæ•´çš„åˆ†æé…ç½®
    """
    # ä»DEFAULT_CONFIGå¼€å§‹ï¼Œå®Œå…¨å¤åˆ¶webç›®å½•çš„é€»è¾‘
    config = DEFAULT_CONFIG.copy()
    config["llm_provider"] = llm_provider
    config["deep_think_llm"] = deep_model
    config["quick_think_llm"] = quick_model

    # æ ¹æ®ç ”ç©¶æ·±åº¦è°ƒæ•´é…ç½® - æ–¹æ¡ˆCï¼šè‡ªå®šä¹‰æ˜ å°„
    if research_depth == "å¿«é€Ÿ":
        config["max_debate_rounds"] = 1
        config["max_risk_discuss_rounds"] = 1
        config["memory_enabled"] = True
        config["online_tools"] = True
        logger.info(f"ğŸ”§ [å¿«é€Ÿåˆ†æ] {market_type}ä½¿ç”¨ç»Ÿä¸€å·¥å…·ï¼Œç¡®ä¿æ•°æ®æºæ­£ç¡®å’Œç¨³å®šæ€§")

        # æ ¹æ®ä¾›åº”å•†ä¼˜åŒ–æ¨¡å‹é€‰æ‹©
        if llm_provider == "dashscope":
            config["quick_think_llm"] = "qwen-turbo"  # ä½¿ç”¨æœ€å¿«æ¨¡å‹
            config["deep_think_llm"] = "qwen-plus"
        elif llm_provider == "deepseek":
            config["quick_think_llm"] = "deepseek-chat"
            config["deep_think_llm"] = "deepseek-chat"

    elif research_depth == "æ ‡å‡†":
        config["max_debate_rounds"] = 1
        config["max_risk_discuss_rounds"] = 2
        config["memory_enabled"] = True
        config["online_tools"] = True

        if llm_provider == "dashscope":
            config["quick_think_llm"] = "qwen-plus"
            config["deep_think_llm"] = "qwen-max"
        elif llm_provider == "deepseek":
            config["quick_think_llm"] = "deepseek-chat"
            config["deep_think_llm"] = "deepseek-chat"

    elif research_depth == "æ·±åº¦":
        config["max_debate_rounds"] = 2
        config["max_risk_discuss_rounds"] = 3
        config["memory_enabled"] = True
        config["online_tools"] = True

        if llm_provider == "dashscope":
            config["quick_think_llm"] = "qwen-max"
            config["deep_think_llm"] = "qwen-max"
        elif llm_provider == "deepseek":
            config["quick_think_llm"] = "deepseek-chat"
            config["deep_think_llm"] = "deepseek-chat"

    # æ ¹æ®LLMæä¾›å•†è®¾ç½®åç«¯URL
    if llm_provider == "dashscope":
        config["backend_url"] = "https://dashscope.aliyuncs.com/api/v1"
    elif llm_provider == "deepseek":
        config["backend_url"] = "https://api.deepseek.com"
    elif llm_provider == "openai":
        config["backend_url"] = "https://api.openai.com/v1"
    elif llm_provider == "google":
        config["backend_url"] = "https://generativelanguage.googleapis.com/v1"
    elif llm_provider == "qianfan":
        config["backend_url"] = "https://aip.baidubce.com"

    # æ·»åŠ åˆ†æå¸ˆé…ç½®
    config["selected_analysts"] = selected_analysts
    config["debug"] = False

    logger.info(f"ğŸ“‹ åˆ›å»ºåˆ†æé…ç½®å®Œæˆ:")
    logger.info(f"   ç ”ç©¶æ·±åº¦: {research_depth}")
    logger.info(f"   è¾©è®ºè½®æ¬¡: {config['max_debate_rounds']}")
    logger.info(f"   é£é™©è®¨è®ºè½®æ¬¡: {config['max_risk_discuss_rounds']}")
    logger.info(f"   LLMä¾›åº”å•†: {llm_provider}")
    logger.info(f"   å¿«é€Ÿæ¨¡å‹: {config['quick_think_llm']}")
    logger.info(f"   æ·±åº¦æ¨¡å‹: {config['deep_think_llm']}")

    return config


class SimpleAnalysisService:
    """ç®€åŒ–çš„è‚¡ç¥¨åˆ†ææœåŠ¡ç±»"""

    def __init__(self):
        self._trading_graph_cache = {}
        self.memory_manager = get_memory_state_manager()

        logger.info(f"ğŸ”§ [æœåŠ¡åˆå§‹åŒ–] SimpleAnalysisService å®ä¾‹ID: {id(self)}")
        logger.info(f"ğŸ”§ [æœåŠ¡åˆå§‹åŒ–] å†…å­˜ç®¡ç†å™¨å®ä¾‹ID: {id(self.memory_manager)}")

        # è®¾ç½® WebSocket ç®¡ç†å™¨
        try:
            from app.services.websocket_manager import get_websocket_manager
            self.memory_manager.set_websocket_manager(get_websocket_manager())
        except ImportError:
            logger.warning("âš ï¸ WebSocket ç®¡ç†å™¨ä¸å¯ç”¨")
    
    def _convert_user_id(self, user_id: str) -> PyObjectId:
        """å°†å­—ç¬¦ä¸²ç”¨æˆ·IDè½¬æ¢ä¸ºPyObjectId"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ç”¨æˆ·ID: {user_id} (ç±»å‹: {type(user_id)})")
            
            # å¦‚æœæ˜¯adminç”¨æˆ·ï¼Œä½¿ç”¨å›ºå®šçš„ObjectId
            if user_id == "admin":
                admin_object_id = ObjectId("507f1f77bcf86cd799439011")
                logger.info(f"ğŸ”„ è½¬æ¢adminç”¨æˆ·ID: {user_id} -> {admin_object_id}")
                return PyObjectId(admin_object_id)
            else:
                # å°è¯•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºObjectId
                object_id = ObjectId(user_id)
                logger.info(f"ğŸ”„ è½¬æ¢ç”¨æˆ·ID: {user_id} -> {object_id}")
                return PyObjectId(object_id)
        except Exception as e:
            logger.error(f"âŒ ç”¨æˆ·IDè½¬æ¢å¤±è´¥: {user_id} -> {e}")
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ObjectId
            new_object_id = ObjectId()
            logger.warning(f"âš ï¸ ç”Ÿæˆæ–°çš„ç”¨æˆ·ID: {new_object_id}")
            return PyObjectId(new_object_id)
    
    def _get_trading_graph(self, config: Dict[str, Any]) -> TradingAgentsGraph:
        """è·å–æˆ–åˆ›å»ºTradingAgentså®ä¾‹ - å®Œå…¨å¤åˆ¶webç›®å½•çš„åˆ›å»ºæ–¹å¼"""
        config_key = str(sorted(config.items()))

        if config_key not in self._trading_graph_cache:
            logger.info(f"åˆ›å»ºæ–°çš„TradingAgentså®ä¾‹...")

            # ç›´æ¥ä½¿ç”¨å®Œæ•´é…ç½®ï¼Œä¸å†åˆå¹¶DEFAULT_CONFIGï¼ˆå› ä¸ºcreate_analysis_configå·²ç»å¤„ç†äº†ï¼‰
            # è¿™ä¸webç›®å½•çš„æ–¹å¼ä¸€è‡´
            self._trading_graph_cache[config_key] = TradingAgentsGraph(
                selected_analysts=config.get("selected_analysts", ["market", "fundamentals"]),
                debug=config.get("debug", False),
                config=config
            )

            logger.info(f"âœ… TradingAgentså®ä¾‹åˆ›å»ºæˆåŠŸ")

        return self._trading_graph_cache[config_key]

    async def create_analysis_task(
        self,
        user_id: str,
        request: SingleAnalysisRequest
    ) -> Dict[str, Any]:
        """åˆ›å»ºåˆ†æä»»åŠ¡ï¼ˆç«‹å³è¿”å›ï¼Œä¸æ‰§è¡Œåˆ†æï¼‰"""
        try:
            # ç”Ÿæˆä»»åŠ¡ID
            task_id = str(uuid.uuid4())

            logger.info(f"ğŸ“ åˆ›å»ºåˆ†æä»»åŠ¡: {task_id} - {request.stock_code}")
            logger.info(f"ğŸ” å†…å­˜ç®¡ç†å™¨å®ä¾‹ID: {id(self.memory_manager)}")

            # åœ¨å†…å­˜ä¸­åˆ›å»ºä»»åŠ¡çŠ¶æ€
            task_state = await self.memory_manager.create_task(
                task_id=task_id,
                user_id=user_id,
                stock_code=request.stock_code,
                parameters=request.parameters.model_dump() if request.parameters else {}
            )

            logger.info(f"âœ… ä»»åŠ¡çŠ¶æ€å·²åˆ›å»º: {task_state.task_id}")

            # ç«‹å³éªŒè¯ä»»åŠ¡æ˜¯å¦å¯ä»¥æŸ¥è¯¢åˆ°
            verify_task = await self.memory_manager.get_task(task_id)
            if verify_task:
                logger.info(f"âœ… ä»»åŠ¡åˆ›å»ºéªŒè¯æˆåŠŸ: {verify_task.task_id}")
            else:
                logger.error(f"âŒ ä»»åŠ¡åˆ›å»ºéªŒè¯å¤±è´¥: æ— æ³•æŸ¥è¯¢åˆ°åˆšåˆ›å»ºçš„ä»»åŠ¡ {task_id}")

            return {
                "task_id": task_id,
                "status": "pending",
                "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…æ‰§è¡Œ"
            }

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºåˆ†æä»»åŠ¡å¤±è´¥: {e}")
            raise

    async def execute_analysis_background(
        self,
        task_id: str,
        user_id: str,
        request: SingleAnalysisRequest
    ):
        """åœ¨åå°æ‰§è¡Œåˆ†æä»»åŠ¡"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹åå°æ‰§è¡Œåˆ†æä»»åŠ¡: {task_id}")

            # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
            await self.memory_manager.update_task_status(
                task_id=task_id,
                status=TaskStatus.RUNNING,
                progress=10,
                message="åˆ†æå¼€å§‹...",
                current_step="initialization"
            )

            # æ•°æ®å‡†å¤‡é˜¶æ®µ
            await self.memory_manager.update_task_status(
                task_id=task_id,
                status=TaskStatus.RUNNING,
                progress=20,
                message="å‡†å¤‡åˆ†ææ•°æ®...",
                current_step="data_preparation"
            )

            # æ‰§è¡Œå®é™…çš„åˆ†æ
            result = await self._execute_analysis_sync(task_id, user_id, request)

            # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
            await self.memory_manager.update_task_status(
                task_id=task_id,
                status=TaskStatus.COMPLETED,
                progress=100,
                message="åˆ†æå®Œæˆ",
                current_step="completed",
                result_data=result
            )

            logger.info(f"âœ… åå°åˆ†æä»»åŠ¡å®Œæˆ: {task_id}")

        except Exception as e:
            logger.error(f"âŒ åå°åˆ†æä»»åŠ¡å¤±è´¥: {task_id} - {e}")

            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            await self.memory_manager.update_task_status(
                task_id=task_id,
                status=TaskStatus.FAILED,
                progress=0,
                message="åˆ†æå¤±è´¥",
                current_step="failed",
                error_message=str(e)
            )

    async def _execute_analysis_sync(
        self,
        task_id: str,
        user_id: str,
        request: SingleAnalysisRequest
    ) -> Dict[str, Any]:
        """åŒæ­¥æ‰§è¡Œåˆ†æï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
        import concurrent.futures

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥åˆ†æ
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(
                executor,
                self._run_analysis_sync,
                task_id,
                user_id,
                request
            )
        return result

    def _run_analysis_sync(
        self,
        task_id: str,
        user_id: str,
        request: SingleAnalysisRequest
    ) -> Dict[str, Any]:
        """åŒæ­¥æ‰§è¡Œåˆ†æçš„å…·ä½“å®ç°"""
        try:
            logger.info(f"ğŸ”„ [çº¿ç¨‹æ± ] å¼€å§‹æ‰§è¡Œåˆ†æ: {task_id} - {request.stock_code}")

            # å¼‚æ­¥æ›´æ–°è¿›åº¦ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è°ƒç”¨ï¼‰
            def update_progress_sync(progress: int, message: str, step: str):
                """åœ¨çº¿ç¨‹æ± ä¸­åŒæ­¥æ›´æ–°è¿›åº¦"""
                try:
                    # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ¥æ‰§è¡Œå¼‚æ­¥æ“ä½œ
                    import asyncio
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        loop.run_until_complete(
                            self.memory_manager.update_task_status(
                                task_id=task_id,
                                status=TaskStatus.RUNNING,
                                progress=progress,
                                message=message,
                                current_step=step
                            )
                        )
                    finally:
                        loop.close()
                except Exception as e:
                    logger.warning(f"âš ï¸ è¿›åº¦æ›´æ–°å¤±è´¥: {e}")

            # é…ç½®é˜¶æ®µ
            update_progress_sync(30, "é…ç½®åˆ†æå‚æ•°...", "configuration")

            # åˆ›å»ºåˆ†æé…ç½®
            config = create_analysis_config(
                research_depth=request.parameters.research_depth if request.parameters else 2,
                selected_analysts=request.parameters.selected_analysts if request.parameters else ["market", "fundamentals"],
                quick_model="qwen-turbo",
                deep_model="qwen-plus",
                llm_provider="dashscope",
                market_type="Aè‚¡"
            )

            # åˆå§‹åŒ–åˆ†æå¼•æ“
            update_progress_sync(40, "åˆå§‹åŒ–åˆ†æå¼•æ“...", "engine_initialization")
            trading_graph = self._get_trading_graph(config)

            # å¼€å§‹åˆ†æ
            update_progress_sync(50, "å¼€å§‹è‚¡ç¥¨åˆ†æ...", "analysis_execution")
            start_time = datetime.now()
            analysis_date = datetime.now().strftime("%Y-%m-%d")

            # è°ƒç”¨åˆ†ææ–¹æ³•
            update_progress_sync(60, "æ‰§è¡Œæ™ºèƒ½ä½“åˆ†æ...", "agent_analysis")
            state, decision = trading_graph.propagate(request.stock_code, analysis_date)

            # å¤„ç†ç»“æœ
            update_progress_sync(90, "å¤„ç†åˆ†æç»“æœ...", "result_processing")

            execution_time = (datetime.now() - start_time).total_seconds()

            # æ„å»ºç»“æœ
            result = {
                "analysis_id": str(uuid.uuid4()),
                "stock_code": request.stock_code,
                "analysis_date": analysis_date,
                "summary": decision.get("summary", ""),
                "recommendation": decision.get("recommendation", ""),
                "confidence_score": decision.get("confidence_score", 0.0),
                "risk_level": decision.get("risk_level", "ä¸­ç­‰"),
                "key_points": decision.get("key_points", []),
                "detailed_analysis": decision,
                "execution_time": execution_time,
                "tokens_used": decision.get("tokens_used", 0),
                "state": state
            }

            logger.info(f"âœ… [çº¿ç¨‹æ± ] åˆ†æå®Œæˆ: {task_id} - è€—æ—¶{execution_time:.2f}ç§’")
            return result

        except Exception as e:
            logger.error(f"âŒ [çº¿ç¨‹æ± ] åˆ†ææ‰§è¡Œå¤±è´¥: {task_id} - {e}")
            raise

    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        logger.info(f"ğŸ” æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€: {task_id}")
        logger.info(f"ğŸ” å½“å‰æœåŠ¡å®ä¾‹ID: {id(self)}")
        logger.info(f"ğŸ” å†…å­˜ç®¡ç†å™¨å®ä¾‹ID: {id(self.memory_manager)}")

        # å¼ºåˆ¶ä½¿ç”¨å…¨å±€å†…å­˜ç®¡ç†å™¨å®ä¾‹ï¼ˆä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼‰
        global_memory_manager = get_memory_state_manager()
        logger.info(f"ğŸ” å…¨å±€å†…å­˜ç®¡ç†å™¨å®ä¾‹ID: {id(global_memory_manager)}")

        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = await global_memory_manager.get_statistics()
        logger.info(f"ğŸ“Š å†…å­˜ä¸­ä»»åŠ¡ç»Ÿè®¡: {stats}")

        result = await global_memory_manager.get_task_dict(task_id)
        if result:
            logger.info(f"âœ… æ‰¾åˆ°ä»»åŠ¡: {task_id} - çŠ¶æ€: {result.get('status')}")
        else:
            logger.warning(f"âŒ æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")

        return result

    async def list_user_tasks(
        self,
        user_id: str,
        status: Optional[str] = None,
        limit: int = 20,
        offset: int = 0
    ) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·ä»»åŠ¡åˆ—è¡¨"""
        task_status = None
        if status:
            try:
                task_status = TaskStatus(status)
            except ValueError:
                pass

        return await self.memory_manager.list_user_tasks(
            user_id=user_id,
            status=task_status,
            limit=limit,
            offset=offset
        )

    async def submit_single_analysis(
        self, 
        user_id: str, 
        request: SingleAnalysisRequest
    ) -> Dict[str, Any]:
        """æäº¤å•è‚¡åˆ†æä»»åŠ¡"""
        try:
            logger.info(f"ğŸ“ å¼€å§‹æäº¤å•è‚¡åˆ†æä»»åŠ¡")
            logger.info(f"ğŸ‘¤ ç”¨æˆ·ID: {user_id}")
            logger.info(f"ğŸ“Š è‚¡ç¥¨ä»£ç : {request.stock_code}")
            logger.info(f"âš™ï¸ åˆ†æå‚æ•°: {request.parameters}")
            
            # ç”Ÿæˆä»»åŠ¡ID
            task_id = str(uuid.uuid4())
            logger.info(f"ğŸ†” ç”Ÿæˆä»»åŠ¡ID: {task_id}")
            
            # è½¬æ¢ç”¨æˆ·ID
            converted_user_id = self._convert_user_id(user_id)
            
            # åˆ›å»ºåˆ†æä»»åŠ¡
            task = AnalysisTask(
                task_id=task_id,
                user_id=converted_user_id,
                stock_code=request.stock_code,
                parameters=request.parameters or AnalysisParameters(),
                status=AnalysisStatus.PENDING
            )
            
            # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
            logger.info(f"ğŸ’¾ ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“...")
            db = get_mongo_db()
            task_dict = task.model_dump(by_alias=True)
            await db.analysis_tasks.insert_one(task_dict)
            logger.info(f"âœ… ä»»åŠ¡å·²ä¿å­˜åˆ°æ•°æ®åº“")
            
            # ç›´æ¥æ‰§è¡Œåˆ†æï¼ˆå¼‚æ­¥ï¼‰
            logger.info(f"ğŸš€ å¼€å§‹ç›´æ¥æ‰§è¡Œå•è‚¡åˆ†æ...")
            asyncio.create_task(self._execute_analysis_async(task))
            
            logger.info(f"ğŸ‰ å•è‚¡åˆ†æä»»åŠ¡æäº¤å®Œæˆ: {task_id} - {request.stock_code}")
            
            return {
                "task_id": task_id,
                "stock_code": request.stock_code,
                "status": AnalysisStatus.PENDING,
                "message": "ä»»åŠ¡å·²æäº¤ï¼Œæ­£åœ¨åå°æ‰§è¡Œ"
            }
            
        except Exception as e:
            logger.error(f"âŒ æäº¤å•è‚¡åˆ†æä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def _execute_analysis_async(self, task: AnalysisTask):
        """å¼‚æ­¥æ‰§è¡Œåˆ†æä»»åŠ¡"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.stock_code}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
            await self._update_task_status(task.task_id, AnalysisStatus.PROCESSING, 10)
            
            # å‡†å¤‡åˆ†æé…ç½®
            # æ ¹æ®ç ”ç©¶æ·±åº¦é€‰æ‹©ä¸åŒçš„æ¨¡å‹é…ç½®
            research_depth = task.parameters.research_depth or "3 çº§ - æ ‡å‡†åˆ†æ"

            # ä»ä»»åŠ¡å‚æ•°ä¸­è·å–æ¨¡å‹é…ç½®ï¼ˆå‰ç«¯ä¼ é€’ï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»ç³»ç»Ÿé…ç½®è·å–
            from app.core.unified_config import unified_config

            quick_model = getattr(task.parameters, 'quick_analysis_model', None) or unified_config.get_quick_analysis_model()
            deep_model = getattr(task.parameters, 'deep_analysis_model', None) or unified_config.get_deep_analysis_model()

            # æ ¹æ®æ¨¡å‹åç§°åŠ¨æ€æŸ¥æ‰¾ä¾›åº”å•†
            llm_provider = await get_provider_by_model_name(quick_model)

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½® - å®Œå…¨å¤åˆ¶webç›®å½•çš„é€»è¾‘
            config = create_analysis_config(
                research_depth=research_depth,
                selected_analysts=task.parameters.selected_analysts or ["market", "fundamentals"],
                quick_model=quick_model,
                deep_model=deep_model,
                llm_provider=llm_provider,
                market_type=task.parameters.market_type or "Aè‚¡"
            )

            logger.info(f"ğŸ“‹ åˆ†æé…ç½®: {config}")
            
            # è·å–TradingAgentså®ä¾‹
            trading_graph = self._get_trading_graph(config)
            
            # æ›´æ–°è¿›åº¦
            await self._update_task_status(task.task_id, AnalysisStatus.PROCESSING, 30)
            
            # æ‰§è¡Œåˆ†æ
            logger.info(f"ğŸ“ˆ å¼€å§‹æ‰§è¡Œè‚¡ç¥¨åˆ†æ: {task.stock_code}")
            analysis_date = task.parameters.analysis_date or datetime.now().strftime("%Y-%m-%d")
            
            # è°ƒç”¨TradingAgentsçš„propagateæ–¹æ³•
            logger.info(f"ğŸ” å¼€å§‹è°ƒç”¨ trading_graph.propagate({task.stock_code}, {analysis_date})")
            logger.info(f"ğŸ” é€‰ä¸­çš„åˆ†æå¸ˆ: {config.get('selected_analysts', [])}")
            logger.info(f"ğŸ” é¢„æœŸåˆ†ææµç¨‹: åˆ†æå¸ˆ â†’ ç ”ç©¶å‘˜è¾©è®º â†’ äº¤æ˜“å‘˜ â†’ é£é™©è¯„ä¼° â†’ ä¿¡å·å¤„ç†")

            start_time = datetime.utcnow()
            final_state, decision = trading_graph.propagate(task.stock_code, analysis_date)
            execution_time = (datetime.utcnow() - start_time).total_seconds()

            logger.info(f"ğŸ” propagate æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {execution_time:.2f}ç§’")
            logger.info(f"ğŸ” è¿”å›çš„ final_state ç±»å‹: {type(final_state)}")
            logger.info(f"ğŸ” è¿”å›çš„ decision ç±»å‹: {type(decision)}")
            if isinstance(decision, dict):
                logger.info(f"ğŸ” decision å†…å®¹: {decision}")
            else:
                logger.info(f"ğŸ” decision å†…å®¹: {str(decision)[:200]}...")
            
            # æ›´æ–°è¿›åº¦
            await self._update_task_status(task.task_id, AnalysisStatus.PROCESSING, 80)
            
            # å¤„ç†åˆ†æç»“æœ - å®Œå…¨é‡‡ç”¨webç›®å½•çš„ç»“æ„
            result = {
                'stock_symbol': task.stock_code,
                'analysis_date': analysis_date,
                'analysts': task.parameters.selected_analysts,
                'research_depth': task.parameters.research_depth,
                'llm_provider': config.get("llm_provider"),
                'llm_model': config.get("quick_think_llm"),
                'state': final_state,
                'decision': decision,
                'success': True,
                'error': None,
                'task_id': task.task_id
            }

            # ä¿å­˜ç»“æœ - å®Œå…¨é‡‡ç”¨webç›®å½•çš„åŒé‡ä¿å­˜æ–¹å¼
            await self._save_analysis_results_complete(task.task_id, result)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            await self._update_task_status(task.task_id, AnalysisStatus.COMPLETED, 100)
            
            logger.info(f"âœ… åˆ†æä»»åŠ¡å®Œæˆ: {task.task_id}")
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            await self._update_task_status(task.task_id, AnalysisStatus.FAILED, 0, str(e))
    
    async def _update_task_status(
        self, 
        task_id: str, 
        status: AnalysisStatus, 
        progress: int, 
        error_message: str = None
    ):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        try:
            db = get_mongo_db()
            update_data = {
                "status": status,
                "progress": progress,
                "updated_at": datetime.utcnow()
            }
            
            if status == AnalysisStatus.PROCESSING and progress == 10:
                update_data["started_at"] = datetime.utcnow()
            elif status == AnalysisStatus.COMPLETED:
                update_data["completed_at"] = datetime.utcnow()
            elif status == AnalysisStatus.FAILED:
                update_data["last_error"] = error_message
                update_data["completed_at"] = datetime.utcnow()
            
            await db.analysis_tasks.update_one(
                {"task_id": task_id},
                {"$set": update_data}
            )
            
            logger.debug(f"ğŸ“Š ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°: {task_id} -> {status} ({progress}%)")
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_id} - {e}")
    
    async def _save_analysis_result(self, task_id: str, result: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœï¼ˆåŸå§‹æ–¹æ³•ï¼‰"""
        try:
            db = get_mongo_db()
            await db.analysis_tasks.update_one(
                {"task_id": task_id},
                {"$set": {"result": result}}
            )
            logger.debug(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜: {task_id}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {task_id} - {e}")

    async def _save_analysis_result_web_style(self, task_id: str, result: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœ - é‡‡ç”¨webç›®å½•çš„æ–¹å¼ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜"""
        try:
            db = get_mongo_db()

            # åˆ›å»ºå¯åºåˆ—åŒ–çš„ç»“æœå‰¯æœ¬
            serializable_result = {}

            # å¤åˆ¶åŸºæœ¬å­—æ®µ
            for key, value in result.items():
                if key in ['stock_symbol', 'analysis_date', 'analysts', 'research_depth',
                          'llm_provider', 'llm_model', 'success', 'error', 'task_id']:
                    serializable_result[key] = value

            # å¤„ç†stateå’Œdecision - è½¬æ¢ä¸ºå­—ç¬¦ä¸²æˆ–æå–å…³é”®ä¿¡æ¯
            if 'state' in result:
                try:
                    # å°è¯•æå–stateä¸­çš„å…³é”®ä¿¡æ¯
                    state = result['state']
                    if hasattr(state, '__dict__'):
                        # å¦‚æœæ˜¯å¯¹è±¡ï¼Œæå–å…¶å­—å…¸è¡¨ç¤º
                        serializable_result['state_summary'] = str(state)
                    else:
                        serializable_result['state_summary'] = str(state)
                except Exception as e:
                    logger.warning(f"âš ï¸ å¤„ç†stateæ—¶å‡ºé”™: {e}")
                    serializable_result['state_summary'] = "åˆ†æçŠ¶æ€å¤„ç†å¤±è´¥"

            if 'decision' in result:
                try:
                    # decisioné€šå¸¸æ˜¯å­—å…¸ï¼Œç›´æ¥ä¿å­˜
                    decision = result['decision']
                    if isinstance(decision, dict):
                        serializable_result['decision'] = decision
                    else:
                        serializable_result['decision'] = str(decision)
                except Exception as e:
                    logger.warning(f"âš ï¸ å¤„ç†decisionæ—¶å‡ºé”™: {e}")
                    serializable_result['decision'] = "å†³ç­–ä¿¡æ¯å¤„ç†å¤±è´¥"

            # æ·»åŠ æ—¶é—´æˆ³
            serializable_result['completed_at'] = datetime.utcnow().isoformat()

            # ä¿å­˜åˆ°æ•°æ®åº“
            await db.analysis_tasks.update_one(
                {"task_id": task_id},
                {"$set": {"result": serializable_result}}
            )
            logger.info(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜ (webé£æ ¼): {task_id}")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {task_id} - {e}")
            # é™çº§åˆ°ç®€å•ä¿å­˜
            try:
                simple_result = {
                    'task_id': task_id,
                    'success': result.get('success', True),
                    'error': str(e),
                    'completed_at': datetime.utcnow().isoformat()
                }
                await db.analysis_tasks.update_one(
                    {"task_id": task_id},
                    {"$set": {"result": simple_result}}
                )
                logger.info(f"ğŸ’¾ ä½¿ç”¨ç®€åŒ–ç»“æœä¿å­˜: {task_id}")
            except Exception as fallback_error:
                logger.error(f"âŒ ç®€åŒ–ä¿å­˜ä¹Ÿå¤±è´¥: {task_id} - {fallback_error}")

    async def _save_analysis_results_complete(self, task_id: str, result: Dict[str, Any]):
        """å®Œæ•´çš„åˆ†æç»“æœä¿å­˜ - å®Œå…¨é‡‡ç”¨webç›®å½•çš„åŒé‡ä¿å­˜æ–¹å¼"""
        try:
            stock_symbol = result.get('stock_symbol', 'UNKNOWN')
            logger.info(f"ğŸ’¾ å¼€å§‹å®Œæ•´ä¿å­˜åˆ†æç»“æœ: {stock_symbol}")

            # 1. ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šåˆ°æœ¬åœ°ç›®å½•
            logger.info(f"ğŸ“ [æœ¬åœ°ä¿å­˜] å¼€å§‹ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šåˆ°æœ¬åœ°ç›®å½•")
            local_files = await self._save_modular_reports_to_data_dir(result, stock_symbol)
            if local_files:
                logger.info(f"âœ… [æœ¬åœ°ä¿å­˜] å·²ä¿å­˜ {len(local_files)} ä¸ªæœ¬åœ°æŠ¥å‘Šæ–‡ä»¶")
                for module, path in local_files.items():
                    logger.info(f"  - {module}: {path}")
            else:
                logger.warning(f"âš ï¸ [æœ¬åœ°ä¿å­˜] æœ¬åœ°æŠ¥å‘Šæ–‡ä»¶ä¿å­˜å¤±è´¥")

            # 2. ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ•°æ®åº“
            logger.info(f"ğŸ—„ï¸ [æ•°æ®åº“ä¿å­˜] å¼€å§‹ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ•°æ®åº“")
            await self._save_analysis_result_web_style(task_id, result)
            logger.info(f"âœ… [æ•°æ®åº“ä¿å­˜] åˆ†ææŠ¥å‘Šå·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“")

            # 3. è®°å½•ä¿å­˜ç»“æœ
            if local_files:
                logger.info(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°æ•°æ®åº“å’Œæœ¬åœ°æ–‡ä»¶")
            else:
                logger.warning(f"âš ï¸ æ•°æ®åº“ä¿å­˜æˆåŠŸï¼Œä½†æœ¬åœ°æ–‡ä»¶ä¿å­˜å¤±è´¥")

        except Exception as save_error:
            logger.error(f"âŒ [å®Œæ•´ä¿å­˜] ä¿å­˜åˆ†ææŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(save_error)}")
            # é™çº§åˆ°ä»…æ•°æ®åº“ä¿å­˜
            try:
                await self._save_analysis_result_web_style(task_id, result)
                logger.info(f"ğŸ’¾ é™çº§ä¿å­˜æˆåŠŸ (ä»…æ•°æ®åº“): {task_id}")
            except Exception as fallback_error:
                logger.error(f"âŒ é™çº§ä¿å­˜ä¹Ÿå¤±è´¥: {task_id} - {fallback_error}")

    async def _save_modular_reports_to_data_dir(self, result: Dict[str, Any], stock_symbol: str) -> Dict[str, str]:
        """ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šåˆ°dataç›®å½• - å®Œå…¨é‡‡ç”¨webç›®å½•çš„æ–‡ä»¶ç»“æ„"""
        try:
            import os
            from pathlib import Path
            from datetime import datetime
            import json

            # è·å–é¡¹ç›®æ ¹ç›®å½•
            project_root = Path(__file__).parent.parent.parent

            # ç¡®å®šresultsç›®å½•è·¯å¾„ - ä¸webç›®å½•ä¿æŒä¸€è‡´
            results_dir_env = os.getenv("TRADINGAGENTS_RESULTS_DIR")
            if results_dir_env:
                if not os.path.isabs(results_dir_env):
                    results_dir = project_root / results_dir_env
                else:
                    results_dir = Path(results_dir_env)
            else:
                # é»˜è®¤ä½¿ç”¨dataç›®å½•è€Œä¸æ˜¯resultsç›®å½•
                results_dir = project_root / "data" / "analysis_results"

            # åˆ›å»ºè‚¡ç¥¨ä¸“ç”¨ç›®å½• - å®Œå…¨æŒ‰ç…§webç›®å½•çš„ç»“æ„
            analysis_date_raw = result.get('analysis_date', datetime.now())

            # ç¡®ä¿ analysis_date æ˜¯å­—ç¬¦ä¸²æ ¼å¼
            if isinstance(analysis_date_raw, datetime):
                analysis_date_str = analysis_date_raw.strftime('%Y-%m-%d')
            elif isinstance(analysis_date_raw, str):
                # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œæ£€æŸ¥æ ¼å¼
                try:
                    # å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
                    parsed_date = datetime.strptime(analysis_date_raw, '%Y-%m-%d')
                    analysis_date_str = analysis_date_raw
                except ValueError:
                    # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
                    analysis_date_str = datetime.now().strftime('%Y-%m-%d')
            else:
                # å…¶ä»–ç±»å‹ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
                analysis_date_str = datetime.now().strftime('%Y-%m-%d')

            stock_dir = results_dir / stock_symbol / analysis_date_str
            reports_dir = stock_dir / "reports"
            reports_dir.mkdir(parents=True, exist_ok=True)

            # åˆ›å»ºmessage_tool.logæ–‡ä»¶ - ä¸webç›®å½•ä¿æŒä¸€è‡´
            log_file = stock_dir / "message_tool.log"
            log_file.touch(exist_ok=True)

            logger.info(f"ğŸ“ åˆ›å»ºåˆ†æç»“æœç›®å½•: {reports_dir}")
            logger.info(f"ğŸ” [è°ƒè¯•] analysis_date_raw ç±»å‹: {type(analysis_date_raw)}, å€¼: {analysis_date_raw}")
            logger.info(f"ğŸ” [è°ƒè¯•] analysis_date_str: {analysis_date_str}")
            logger.info(f"ğŸ” [è°ƒè¯•] å®Œæ•´è·¯å¾„: {os.path.normpath(str(reports_dir))}")

            state = result.get('state', {})
            saved_files = {}

            # å®šä¹‰æŠ¥å‘Šæ¨¡å—æ˜ å°„ - å®Œå…¨æŒ‰ç…§webç›®å½•çš„å®šä¹‰
            report_modules = {
                'market_report': {
                    'filename': 'market_report.md',
                    'title': f'{stock_symbol} è‚¡ç¥¨æŠ€æœ¯åˆ†ææŠ¥å‘Š',
                    'state_key': 'market_report'
                },
                'sentiment_report': {
                    'filename': 'sentiment_report.md',
                    'title': f'{stock_symbol} å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š',
                    'state_key': 'sentiment_report'
                },
                'news_report': {
                    'filename': 'news_report.md',
                    'title': f'{stock_symbol} æ–°é—»äº‹ä»¶åˆ†ææŠ¥å‘Š',
                    'state_key': 'news_report'
                },
                'fundamentals_report': {
                    'filename': 'fundamentals_report.md',
                    'title': f'{stock_symbol} åŸºæœ¬é¢åˆ†ææŠ¥å‘Š',
                    'state_key': 'fundamentals_report'
                },
                'investment_plan': {
                    'filename': 'investment_plan.md',
                    'title': f'{stock_symbol} æŠ•èµ„å†³ç­–æŠ¥å‘Š',
                    'state_key': 'investment_plan'
                },
                'trader_investment_plan': {
                    'filename': 'trader_investment_plan.md',
                    'title': f'{stock_symbol} äº¤æ˜“è®¡åˆ’æŠ¥å‘Š',
                    'state_key': 'trader_investment_plan'
                },
                'final_trade_decision': {
                    'filename': 'final_trade_decision.md',
                    'title': f'{stock_symbol} æœ€ç»ˆæŠ•èµ„å†³ç­–',
                    'state_key': 'final_trade_decision'
                },
                'investment_debate_state': {
                    'filename': 'research_team_decision.md',
                    'title': f'{stock_symbol} ç ”ç©¶å›¢é˜Ÿå†³ç­–æŠ¥å‘Š',
                    'state_key': 'investment_debate_state'
                },
                'risk_debate_state': {
                    'filename': 'risk_management_decision.md',
                    'title': f'{stock_symbol} é£é™©ç®¡ç†å›¢é˜Ÿå†³ç­–æŠ¥å‘Š',
                    'state_key': 'risk_debate_state'
                }
            }

            # ä¿å­˜å„æ¨¡å—æŠ¥å‘Š - å®Œå…¨æŒ‰ç…§webç›®å½•çš„æ–¹å¼
            for module_key, module_info in report_modules.items():
                try:
                    state_key = module_info['state_key']
                    if state_key in state:
                        # æå–æ¨¡å—å†…å®¹
                        module_content = state[state_key]
                        if isinstance(module_content, str):
                            report_content = module_content
                        else:
                            report_content = str(module_content)

                        # ä¿å­˜åˆ°æ–‡ä»¶ - ä½¿ç”¨webç›®å½•çš„æ–‡ä»¶å
                        file_path = reports_dir / module_info['filename']
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(report_content)

                        saved_files[module_key] = str(file_path)
                        logger.info(f"âœ… ä¿å­˜æ¨¡å—æŠ¥å‘Š: {file_path}")

                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜æ¨¡å— {module_key} å¤±è´¥: {e}")

            # ä¿å­˜æœ€ç»ˆå†³ç­–æŠ¥å‘Š - å®Œå…¨æŒ‰ç…§webç›®å½•çš„æ–¹å¼
            decision = result.get('decision', {})
            if decision:
                decision_content = f"# {stock_symbol} æœ€ç»ˆæŠ•èµ„å†³ç­–\n\n"

                if isinstance(decision, dict):
                    decision_content += f"## æŠ•èµ„å»ºè®®\n\n"
                    decision_content += f"**è¡ŒåŠ¨**: {decision.get('action', 'N/A')}\n\n"
                    decision_content += f"**ç½®ä¿¡åº¦**: {decision.get('confidence', 0):.1%}\n\n"
                    decision_content += f"**é£é™©è¯„åˆ†**: {decision.get('risk_score', 0):.1%}\n\n"
                    decision_content += f"**ç›®æ ‡ä»·ä½**: {decision.get('target_price', 'N/A')}\n\n"
                    decision_content += f"## åˆ†ææ¨ç†\n\n{decision.get('reasoning', 'æš‚æ— åˆ†ææ¨ç†')}\n\n"
                else:
                    decision_content += f"{str(decision)}\n\n"

                decision_file = reports_dir / "final_trade_decision.md"
                with open(decision_file, 'w', encoding='utf-8') as f:
                    f.write(decision_content)

                saved_files['final_trade_decision'] = str(decision_file)
                logger.info(f"âœ… ä¿å­˜æœ€ç»ˆå†³ç­–: {decision_file}")

            # ä¿å­˜åˆ†æå…ƒæ•°æ®æ–‡ä»¶ - å®Œå…¨æŒ‰ç…§webç›®å½•çš„æ–¹å¼
            metadata = {
                'stock_symbol': stock_symbol,
                'analysis_date': analysis_date_str,
                'timestamp': datetime.now().isoformat(),
                'research_depth': result.get('research_depth', 1),
                'analysts': result.get('analysts', []),
                'status': 'completed',
                'reports_count': len(saved_files),
                'report_types': list(saved_files.keys())
            }

            metadata_file = reports_dir.parent / "analysis_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… ä¿å­˜åˆ†æå…ƒæ•°æ®: {metadata_file}")
            logger.info(f"âœ… åˆ†æ¨¡å—æŠ¥å‘Šä¿å­˜å®Œæˆï¼Œå…±ä¿å­˜ {len(saved_files)} ä¸ªæ–‡ä»¶")
            logger.info(f"ğŸ“ ä¿å­˜ç›®å½•: {os.path.normpath(str(reports_dir))}")

            return saved_files

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šå¤±è´¥: {e}")
            import traceback
            logger.error(f"âŒ è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {}
    
# é‡å¤çš„ get_task_status æ–¹æ³•å·²åˆ é™¤ï¼Œä½¿ç”¨ç¬¬469è¡Œçš„å†…å­˜ç‰ˆæœ¬


# å…¨å±€æœåŠ¡å®ä¾‹
_analysis_service = None

def get_simple_analysis_service() -> SimpleAnalysisService:
    """è·å–åˆ†ææœåŠ¡å®ä¾‹"""
    global _analysis_service
    if _analysis_service is None:
        logger.info("ğŸ”§ [å•ä¾‹] åˆ›å»ºæ–°çš„ SimpleAnalysisService å®ä¾‹")
        _analysis_service = SimpleAnalysisService()
    else:
        logger.info(f"ğŸ”§ [å•ä¾‹] è¿”å›ç°æœ‰çš„ SimpleAnalysisService å®ä¾‹: {id(_analysis_service)}")
    return _analysis_service
