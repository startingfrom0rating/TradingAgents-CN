"""
AKShareæ•°æ®åŒæ­¥æœåŠ¡
åŸºäºAKShareæä¾›å™¨çš„ç»Ÿä¸€æ•°æ®åŒæ­¥æ–¹æ¡ˆ
"""
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional

from app.core.database import get_mongo_db
from app.services.historical_data_service import get_historical_data_service
from tradingagents.dataflows.providers.akshare_provider import AKShareProvider

logger = logging.getLogger(__name__)


class AKShareSyncService:
    """
    AKShareæ•°æ®åŒæ­¥æœåŠ¡
    
    æä¾›å®Œæ•´çš„æ•°æ®åŒæ­¥åŠŸèƒ½ï¼š
    - è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥
    - å®æ—¶è¡Œæƒ…åŒæ­¥
    - å†å²æ•°æ®åŒæ­¥
    - è´¢åŠ¡æ•°æ®åŒæ­¥
    """
    
    def __init__(self):
        self.provider = None
        self.historical_service = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.db = None
        self.batch_size = 100
        self.rate_limit_delay = 0.2  # AKShareå»ºè®®çš„å»¶è¿Ÿ
    
    async def initialize(self):
        """åˆå§‹åŒ–åŒæ­¥æœåŠ¡"""
        try:
            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            self.db = get_mongo_db()

            # åˆå§‹åŒ–å†å²æ•°æ®æœåŠ¡
            self.historical_service = await get_historical_data_service()

            # åˆå§‹åŒ–AKShareæä¾›å™¨
            self.provider = AKShareProvider()

            # æµ‹è¯•è¿æ¥
            if not await self.provider.test_connection():
                raise RuntimeError("âŒ AKShareè¿æ¥å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨åŒæ­¥æœåŠ¡")

            logger.info("âœ… AKShareåŒæ­¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ AKShareåŒæ­¥æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def sync_stock_basic_info(self, force_update: bool = False) -> Dict[str, Any]:
        """
        åŒæ­¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        
        Args:
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
            
        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯...")
        
        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "start_time": datetime.utcnow(),
            "end_time": None,
            "duration": 0,
            "errors": []
        }
        
        try:
            # 1. è·å–è‚¡ç¥¨åˆ—è¡¨
            stock_list = await self.provider.get_stock_list()
            if not stock_list:
                logger.warning("âš ï¸ æœªè·å–åˆ°è‚¡ç¥¨åˆ—è¡¨")
                return stats
            
            stats["total_processed"] = len(stock_list)
            logger.info(f"ğŸ“Š è·å–åˆ° {len(stock_list)} åªè‚¡ç¥¨ä¿¡æ¯")
            
            # 2. æ‰¹é‡å¤„ç†
            for i in range(0, len(stock_list), self.batch_size):
                batch = stock_list[i:i + self.batch_size]
                batch_stats = await self._process_basic_info_batch(batch, force_update)
                
                # æ›´æ–°ç»Ÿè®¡
                stats["success_count"] += batch_stats["success_count"]
                stats["error_count"] += batch_stats["error_count"]
                stats["skipped_count"] += batch_stats["skipped_count"]
                stats["errors"].extend(batch_stats["errors"])
                
                # è¿›åº¦æ—¥å¿—
                progress = min(i + self.batch_size, len(stock_list))
                logger.info(f"ğŸ“ˆ åŸºç¡€ä¿¡æ¯åŒæ­¥è¿›åº¦: {progress}/{len(stock_list)} "
                           f"(æˆåŠŸ: {stats['success_count']}, é”™è¯¯: {stats['error_count']})")
                
                # APIé™æµ
                if i + self.batch_size < len(stock_list):
                    await asyncio.sleep(self.rate_limit_delay)
            
            # 3. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()
            
            logger.info(f"ğŸ‰ è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥å®Œæˆï¼")
            logger.info(f"ğŸ“Š æ€»è®¡: {stats['total_processed']}åª, "
                       f"æˆåŠŸ: {stats['success_count']}, "
                       f"é”™è¯¯: {stats['error_count']}, "
                       f"è·³è¿‡: {stats['skipped_count']}, "
                       f"è€—æ—¶: {stats['duration']:.2f}ç§’")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_stock_basic_info"})
            return stats
    
    async def _process_basic_info_batch(self, batch: List[Dict[str, Any]], force_update: bool) -> Dict[str, Any]:
        """å¤„ç†åŸºç¡€ä¿¡æ¯æ‰¹æ¬¡"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "errors": []
        }
        
        for stock_info in batch:
            try:
                code = stock_info["code"]
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                if not force_update:
                    existing = await self.db.stock_basic_info.find_one({"code": code})
                    if existing and self._is_data_fresh(existing.get("updated_at"), hours=24):
                        batch_stats["skipped_count"] += 1
                        continue
                
                # è·å–è¯¦ç»†åŸºç¡€ä¿¡æ¯
                basic_info = await self.provider.get_stock_basic_info(code)
                
                if basic_info:
                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                    if hasattr(basic_info, 'model_dump'):
                        basic_data = basic_info.model_dump()
                    elif hasattr(basic_info, 'dict'):
                        basic_data = basic_info.dict()
                    else:
                        basic_data = basic_info
                    
                    # æ›´æ–°åˆ°æ•°æ®åº“
                    try:
                        await self.db.stock_basic_info.update_one(
                            {"code": code},
                            {"$set": basic_data},
                            upsert=True
                        )
                        batch_stats["success_count"] += 1
                    except Exception as e:
                        batch_stats["error_count"] += 1
                        batch_stats["errors"].append({
                            "code": code,
                            "error": f"æ•°æ®åº“æ›´æ–°å¤±è´¥: {str(e)}",
                            "context": "update_stock_basic_info"
                        })
                else:
                    batch_stats["error_count"] += 1
                    batch_stats["errors"].append({
                        "code": code,
                        "error": "è·å–åŸºç¡€ä¿¡æ¯å¤±è´¥",
                        "context": "get_stock_basic_info"
                    })
                
            except Exception as e:
                batch_stats["error_count"] += 1
                batch_stats["errors"].append({
                    "code": stock_info.get("code", "unknown"),
                    "error": str(e),
                    "context": "_process_basic_info_batch"
                })
        
        return batch_stats
    
    def _is_data_fresh(self, updated_at: Any, hours: int = 24) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦æ–°é²œ"""
        if not updated_at:
            return False
        
        try:
            if isinstance(updated_at, str):
                updated_at = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))
            elif isinstance(updated_at, datetime):
                pass
            else:
                return False
            
            # è½¬æ¢ä¸ºUTCæ—¶é—´è¿›è¡Œæ¯”è¾ƒ
            if updated_at.tzinfo is None:
                updated_at = updated_at.replace(tzinfo=None)
            else:
                updated_at = updated_at.replace(tzinfo=None)
            
            now = datetime.utcnow()
            time_diff = now - updated_at
            
            return time_diff.total_seconds() < (hours * 3600)
            
        except Exception as e:
            logger.debug(f"æ£€æŸ¥æ•°æ®æ–°é²œåº¦å¤±è´¥: {e}")
            return False
    
    async def sync_realtime_quotes(self, symbols: List[str] = None) -> Dict[str, Any]:
        """
        åŒæ­¥å®æ—¶è¡Œæƒ…æ•°æ®
        
        Args:
            symbols: æŒ‡å®šè‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™åŒæ­¥æ‰€æœ‰è‚¡ç¥¨
            
        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥å®æ—¶è¡Œæƒ…...")
        
        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "start_time": datetime.utcnow(),
            "end_time": None,
            "duration": 0,
            "errors": []
        }
        
        try:
            # 1. ç¡®å®šè¦åŒæ­¥çš„è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                # ä»æ•°æ®åº“è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
                basic_info_cursor = self.db.stock_basic_info.find({}, {"code": 1})
                symbols = [doc["code"] async for doc in basic_info_cursor]
            
            if not symbols:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è¦åŒæ­¥çš„è‚¡ç¥¨")
                return stats
            
            stats["total_processed"] = len(symbols)
            logger.info(f"ğŸ“Š å‡†å¤‡åŒæ­¥ {len(symbols)} åªè‚¡ç¥¨çš„è¡Œæƒ…")
            
            # 2. æ‰¹é‡å¤„ç†
            for i in range(0, len(symbols), self.batch_size):
                batch = symbols[i:i + self.batch_size]
                batch_stats = await self._process_quotes_batch(batch)
                
                # æ›´æ–°ç»Ÿè®¡
                stats["success_count"] += batch_stats["success_count"]
                stats["error_count"] += batch_stats["error_count"]
                stats["errors"].extend(batch_stats["errors"])
                
                # è¿›åº¦æ—¥å¿—
                progress = min(i + self.batch_size, len(symbols))
                logger.info(f"ğŸ“ˆ è¡Œæƒ…åŒæ­¥è¿›åº¦: {progress}/{len(symbols)} "
                           f"(æˆåŠŸ: {stats['success_count']}, é”™è¯¯: {stats['error_count']})")
                
                # APIé™æµ
                if i + self.batch_size < len(symbols):
                    await asyncio.sleep(self.rate_limit_delay)
            
            # 3. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()
            
            logger.info(f"ğŸ‰ å®æ—¶è¡Œæƒ…åŒæ­¥å®Œæˆï¼")
            logger.info(f"ğŸ“Š æ€»è®¡: {stats['total_processed']}åª, "
                       f"æˆåŠŸ: {stats['success_count']}, "
                       f"é”™è¯¯: {stats['error_count']}, "
                       f"è€—æ—¶: {stats['duration']:.2f}ç§’")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ å®æ—¶è¡Œæƒ…åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_realtime_quotes"})
            return stats
    
    async def _process_quotes_batch(self, batch: List[str]) -> Dict[str, Any]:
        """å¤„ç†è¡Œæƒ…æ‰¹æ¬¡"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "errors": []
        }
        
        # å¹¶å‘è·å–è¡Œæƒ…æ•°æ®
        tasks = []
        for symbol in batch:
            tasks.append(self._get_and_save_quotes(symbol))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                batch_stats["error_count"] += 1
                batch_stats["errors"].append({
                    "code": batch[i],
                    "error": str(result),
                    "context": "_process_quotes_batch"
                })
            elif result:
                batch_stats["success_count"] += 1
            else:
                batch_stats["error_count"] += 1
                batch_stats["errors"].append({
                    "code": batch[i],
                    "error": "è·å–è¡Œæƒ…æ•°æ®å¤±è´¥",
                    "context": "_process_quotes_batch"
                })
        
        return batch_stats
    
    async def _get_and_save_quotes(self, symbol: str) -> bool:
        """è·å–å¹¶ä¿å­˜å•ä¸ªè‚¡ç¥¨è¡Œæƒ…"""
        try:
            quotes = await self.provider.get_stock_quotes(symbol)
            if quotes:
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                if hasattr(quotes, 'model_dump'):
                    quotes_data = quotes.model_dump()
                elif hasattr(quotes, 'dict'):
                    quotes_data = quotes.dict()
                else:
                    quotes_data = quotes

                # æ›´æ–°åˆ°æ•°æ®åº“
                await self.db.market_quotes.update_one(
                    {"code": symbol},
                    {"$set": quotes_data},
                    upsert=True
                )
                return True
            return False
        except Exception as e:
            logger.error(f"âŒ è·å– {symbol} è¡Œæƒ…å¤±è´¥: {e}")
            return False

    async def sync_historical_data(
        self,
        start_date: str = None,
        end_date: str = None,
        symbols: List[str] = None,
        incremental: bool = True
    ) -> Dict[str, Any]:
        """
        åŒæ­¥å†å²æ•°æ®

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            symbols: æŒ‡å®šè‚¡ç¥¨ä»£ç åˆ—è¡¨
            incremental: æ˜¯å¦å¢é‡åŒæ­¥

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥å†å²æ•°æ®...")

        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "total_records": 0,
            "start_time": datetime.utcnow(),
            "end_time": None,
            "duration": 0,
            "errors": []
        }

        try:
            # 1. è®¾ç½®é»˜è®¤æ—¥æœŸèŒƒå›´
            if not end_date:
                end_date = datetime.now().strftime('%Y-%m-%d')
            if not start_date:
                if incremental:
                    # å¢é‡åŒæ­¥ï¼šæœ€è¿‘30å¤©
                    start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
                else:
                    # å…¨é‡åŒæ­¥ï¼šæœ€è¿‘1å¹´
                    start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')

            # 2. ç¡®å®šè¦åŒæ­¥çš„è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                basic_info_cursor = self.db.stock_basic_info.find({}, {"code": 1})
                symbols = [doc["code"] async for doc in basic_info_cursor]

            if not symbols:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è¦åŒæ­¥çš„è‚¡ç¥¨")
                return stats

            stats["total_processed"] = len(symbols)
            logger.info(f"ğŸ“Š å‡†å¤‡åŒæ­¥ {len(symbols)} åªè‚¡ç¥¨çš„å†å²æ•°æ® ({start_date} åˆ° {end_date})")

            # 3. æ‰¹é‡å¤„ç†
            for i in range(0, len(symbols), self.batch_size):
                batch = symbols[i:i + self.batch_size]
                batch_stats = await self._process_historical_batch(batch, start_date, end_date)

                # æ›´æ–°ç»Ÿè®¡
                stats["success_count"] += batch_stats["success_count"]
                stats["error_count"] += batch_stats["error_count"]
                stats["total_records"] += batch_stats["total_records"]
                stats["errors"].extend(batch_stats["errors"])

                # è¿›åº¦æ—¥å¿—
                progress = min(i + self.batch_size, len(symbols))
                logger.info(f"ğŸ“ˆ å†å²æ•°æ®åŒæ­¥è¿›åº¦: {progress}/{len(symbols)} "
                           f"(æˆåŠŸ: {stats['success_count']}, è®°å½•: {stats['total_records']})")

                # APIé™æµ
                if i + self.batch_size < len(symbols):
                    await asyncio.sleep(self.rate_limit_delay)

            # 4. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

            logger.info(f"ğŸ‰ å†å²æ•°æ®åŒæ­¥å®Œæˆï¼")
            logger.info(f"ğŸ“Š æ€»è®¡: {stats['total_processed']}åªè‚¡ç¥¨, "
                       f"æˆåŠŸ: {stats['success_count']}, "
                       f"è®°å½•: {stats['total_records']}æ¡, "
                       f"è€—æ—¶: {stats['duration']:.2f}ç§’")

            return stats

        except Exception as e:
            logger.error(f"âŒ å†å²æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_historical_data"})
            return stats

    async def _process_historical_batch(self, batch: List[str], start_date: str, end_date: str) -> Dict[str, Any]:
        """å¤„ç†å†å²æ•°æ®æ‰¹æ¬¡"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "total_records": 0,
            "errors": []
        }

        for symbol in batch:
            try:
                # è·å–å†å²æ•°æ®
                hist_data = await self.provider.get_historical_data(symbol, start_date, end_date)

                if hist_data is not None and not hist_data.empty:
                    # ä¿å­˜åˆ°ç»Ÿä¸€å†å²æ•°æ®é›†åˆ
                    if self.historical_service is None:
                        self.historical_service = await get_historical_data_service()

                    saved_count = await self.historical_service.save_historical_data(
                        symbol=symbol,
                        data=hist_data,
                        data_source="akshare",
                        market="CN"
                    )

                    batch_stats["success_count"] += 1
                    batch_stats["total_records"] += saved_count
                    logger.debug(f"âœ… {symbol}å†å²æ•°æ®åŒæ­¥æˆåŠŸ: {saved_count}æ¡è®°å½•")
                else:
                    batch_stats["error_count"] += 1
                    batch_stats["errors"].append({
                        "code": symbol,
                        "error": "å†å²æ•°æ®ä¸ºç©º",
                        "context": "_process_historical_batch"
                    })

            except Exception as e:
                batch_stats["error_count"] += 1
                batch_stats["errors"].append({
                    "code": symbol,
                    "error": str(e),
                    "context": "_process_historical_batch"
                })

        return batch_stats

    async def sync_financial_data(self, symbols: List[str] = None) -> Dict[str, Any]:
        """
        åŒæ­¥è´¢åŠ¡æ•°æ®

        Args:
            symbols: æŒ‡å®šè‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥è´¢åŠ¡æ•°æ®...")

        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "start_time": datetime.utcnow(),
            "end_time": None,
            "duration": 0,
            "errors": []
        }

        try:
            # 1. ç¡®å®šè¦åŒæ­¥çš„è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                basic_info_cursor = self.db.stock_basic_info.find({}, {"code": 1})
                symbols = [doc["code"] async for doc in basic_info_cursor]

            if not symbols:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è¦åŒæ­¥çš„è‚¡ç¥¨")
                return stats

            stats["total_processed"] = len(symbols)
            logger.info(f"ğŸ“Š å‡†å¤‡åŒæ­¥ {len(symbols)} åªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®")

            # 2. æ‰¹é‡å¤„ç†
            for i in range(0, len(symbols), self.batch_size):
                batch = symbols[i:i + self.batch_size]
                batch_stats = await self._process_financial_batch(batch)

                # æ›´æ–°ç»Ÿè®¡
                stats["success_count"] += batch_stats["success_count"]
                stats["error_count"] += batch_stats["error_count"]
                stats["errors"].extend(batch_stats["errors"])

                # è¿›åº¦æ—¥å¿—
                progress = min(i + self.batch_size, len(symbols))
                logger.info(f"ğŸ“ˆ è´¢åŠ¡æ•°æ®åŒæ­¥è¿›åº¦: {progress}/{len(symbols)} "
                           f"(æˆåŠŸ: {stats['success_count']}, é”™è¯¯: {stats['error_count']})")

                # APIé™æµ
                if i + self.batch_size < len(symbols):
                    await asyncio.sleep(self.rate_limit_delay)

            # 3. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

            logger.info(f"ğŸ‰ è´¢åŠ¡æ•°æ®åŒæ­¥å®Œæˆï¼")
            logger.info(f"ğŸ“Š æ€»è®¡: {stats['total_processed']}åªè‚¡ç¥¨, "
                       f"æˆåŠŸ: {stats['success_count']}, "
                       f"é”™è¯¯: {stats['error_count']}, "
                       f"è€—æ—¶: {stats['duration']:.2f}ç§’")

            return stats

        except Exception as e:
            logger.error(f"âŒ è´¢åŠ¡æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_financial_data"})
            return stats

    async def _process_financial_batch(self, batch: List[str]) -> Dict[str, Any]:
        """å¤„ç†è´¢åŠ¡æ•°æ®æ‰¹æ¬¡"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "errors": []
        }

        for symbol in batch:
            try:
                # è·å–è´¢åŠ¡æ•°æ®
                financial_data = await self.provider.get_financial_data(symbol)

                if financial_data:
                    # ä½¿ç”¨ç»Ÿä¸€çš„è´¢åŠ¡æ•°æ®æœåŠ¡ä¿å­˜æ•°æ®
                    success = await self._save_financial_data(symbol, financial_data)
                    if success:
                        batch_stats["success_count"] += 1
                        logger.debug(f"âœ… {symbol}è´¢åŠ¡æ•°æ®ä¿å­˜æˆåŠŸ")
                    else:
                        batch_stats["error_count"] += 1
                        batch_stats["errors"].append({
                            "code": symbol,
                            "error": "è´¢åŠ¡æ•°æ®ä¿å­˜å¤±è´¥",
                            "context": "_process_financial_batch"
                        })
                else:
                    batch_stats["error_count"] += 1
                    batch_stats["errors"].append({
                        "code": symbol,
                        "error": "è´¢åŠ¡æ•°æ®ä¸ºç©º",
                        "context": "_process_financial_batch"
                    })

            except Exception as e:
                batch_stats["error_count"] += 1
                batch_stats["errors"].append({
                    "code": symbol,
                    "error": str(e),
                    "context": "_process_financial_batch"
                })

        return batch_stats

    async def _save_financial_data(self, symbol: str, financial_data: Dict[str, Any]) -> bool:
        """ä¿å­˜è´¢åŠ¡æ•°æ®"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„è´¢åŠ¡æ•°æ®æœåŠ¡
            from app.services.financial_data_service import get_financial_data_service

            financial_service = await get_financial_data_service()

            # ä¿å­˜è´¢åŠ¡æ•°æ®
            saved_count = await financial_service.save_financial_data(
                symbol=symbol,
                financial_data=financial_data,
                data_source="akshare",
                market="CN",
                report_type="quarterly"
            )

            return saved_count > 0

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ {symbol} è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
            return False

    async def run_status_check(self) -> Dict[str, Any]:
        """è¿è¡ŒçŠ¶æ€æ£€æŸ¥"""
        try:
            logger.info("ğŸ” å¼€å§‹AKShareçŠ¶æ€æ£€æŸ¥...")

            # æ£€æŸ¥æä¾›å™¨è¿æ¥
            provider_connected = await self.provider.test_connection()

            # æ£€æŸ¥æ•°æ®åº“é›†åˆçŠ¶æ€
            collections_status = {}

            # æ£€æŸ¥åŸºç¡€ä¿¡æ¯é›†åˆ
            basic_count = await self.db.stock_basic_info.count_documents({})
            latest_basic = await self.db.stock_basic_info.find_one(
                {}, sort=[("updated_at", -1)]
            )
            collections_status["stock_basic_info"] = {
                "count": basic_count,
                "latest_update": latest_basic.get("updated_at") if latest_basic else None
            }

            # æ£€æŸ¥è¡Œæƒ…æ•°æ®é›†åˆ
            quotes_count = await self.db.market_quotes.count_documents({})
            latest_quotes = await self.db.market_quotes.find_one(
                {}, sort=[("updated_at", -1)]
            )
            collections_status["market_quotes"] = {
                "count": quotes_count,
                "latest_update": latest_quotes.get("updated_at") if latest_quotes else None
            }

            status_result = {
                "provider_connected": provider_connected,
                "collections": collections_status,
                "status_time": datetime.utcnow()
            }

            logger.info(f"âœ… AKShareçŠ¶æ€æ£€æŸ¥å®Œæˆ: {status_result}")
            return status_result

        except Exception as e:
            logger.error(f"âŒ AKShareçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
            return {
                "provider_connected": False,
                "error": str(e),
                "status_time": datetime.utcnow()
            }


# å…¨å±€åŒæ­¥æœåŠ¡å®ä¾‹
_akshare_sync_service = None

async def get_akshare_sync_service() -> AKShareSyncService:
    """è·å–AKShareåŒæ­¥æœåŠ¡å®ä¾‹"""
    global _akshare_sync_service
    if _akshare_sync_service is None:
        _akshare_sync_service = AKShareSyncService()
        await _akshare_sync_service.initialize()
    return _akshare_sync_service


# APSchedulerå…¼å®¹çš„ä»»åŠ¡å‡½æ•°
async def run_akshare_basic_info_sync(force_update: bool = False):
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯"""
    try:
        service = await get_akshare_sync_service()
        result = await service.sync_stock_basic_info(force_update=force_update)
        logger.info(f"âœ… AKShareåŸºç¡€ä¿¡æ¯åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ AKShareåŸºç¡€ä¿¡æ¯åŒæ­¥å¤±è´¥: {e}")
        raise


async def run_akshare_quotes_sync():
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥å®æ—¶è¡Œæƒ…"""
    try:
        service = await get_akshare_sync_service()
        result = await service.sync_realtime_quotes()
        logger.info(f"âœ… AKShareè¡Œæƒ…åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ AKShareè¡Œæƒ…åŒæ­¥å¤±è´¥: {e}")
        raise


async def run_akshare_historical_sync(incremental: bool = True):
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥å†å²æ•°æ®"""
    try:
        service = await get_akshare_sync_service()
        result = await service.sync_historical_data(incremental=incremental)
        logger.info(f"âœ… AKShareå†å²æ•°æ®åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ AKShareå†å²æ•°æ®åŒæ­¥å¤±è´¥: {e}")
        raise


async def run_akshare_financial_sync():
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥è´¢åŠ¡æ•°æ®"""
    try:
        service = await get_akshare_sync_service()
        result = await service.sync_financial_data()
        logger.info(f"âœ… AKShareè´¢åŠ¡æ•°æ®åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ AKShareè´¢åŠ¡æ•°æ®åŒæ­¥å¤±è´¥: {e}")
        raise


async def run_akshare_status_check():
    """APSchedulerä»»åŠ¡ï¼šçŠ¶æ€æ£€æŸ¥"""
    try:
        service = await get_akshare_sync_service()
        result = await service.run_status_check()
        logger.info(f"âœ… AKShareçŠ¶æ€æ£€æŸ¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ AKShareçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
        raise
